
@article{bay_evaluation_2009,
	title = {{EVALUATION} {OF} {MULTIPLE}-{F0} {ESTIMATION} {AND} {TRACKING} {SYSTEMS}},
	abstract = {Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems. This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed.},
	language = {en},
	journal = {Poster Session},
	author = {Bay, Mert and Ehmann, Andreas F and Downie, J Stephen},
	year = {2009},
	pages = {6},
	file = {Bay et al. - 2009 - EVALUATION OF MULTIPLE-F0 ESTIMATION AND TRACKING .pdf:/home/nathan/Zotero/storage/846V3JUP/Bay et al. - 2009 - EVALUATION OF MULTIPLE-F0 ESTIMATION AND TRACKING .pdf:application/pdf},
}

@article{arthur_songexplorer_2021,
	title = {{SongExplorer}: {A} deep learning workflow for discovery and segmentation of animal acoustic communication signals},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{SongExplorer}},
	url = {https://www.biorxiv.org/content/10.1101/2021.03.26.437280v1},
	doi = {10.1101/2021.03.26.437280},
	abstract = {{\textless}p{\textgreater}Many animals produce distinct sounds or substrate-borne vibrations, but these signals have proved challenging to segment with automated algorithms. We have developed SongExplorer, a web-browser based interface wrapped around a deep-learning algorithm that supports an interactive workflow for (1) discovery of animal sounds, (2) manual annotation, (3) supervised training of a deep convolutional neural network, and (4) automated segmentation of recordings. Raw data can be explored by simultaneously examining song events, both individually and in the context of the entire recording, watching synced video, and listening to song. We provide a simple way to visualize many song events from large datasets within an interactive low-dimensional visualization, which facilitates detection and correction of incorrectly labelled song events. The machine learning model we implemented displays higher accuracy than existing heuristic algorithms and similar accuracy as two expert human annotators. We show that SongExplorer allows rapid detection of all song types from new species and of novel song types in previously well-studied species.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-03-30},
	journal = {bioRxiv},
	author = {Arthur, Benjamin J. and Ding, Yun and Sosale, Medhini and Khalif, Faduma and Kim, Elizabeth and Waddell, Peter and Turaga, Srinivas C. and Stern, David L.},
	month = mar,
	year = {2021},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2021.03.26.437280},
	file = {Snapshot:/home/nathan/Zotero/storage/ETRE8BIU/2021.03.26.html:text/html;Full Text PDF:/home/nathan/Zotero/storage/RTYV4PCA/Arthur et al. - 2021 - SongExplorer A deep learning workflow for discove.pdf:application/pdf},
}

@article{cohen_tweetynet_2020,
	title = {{TweetyNet}: {A} neural network that enables high-throughput, automated annotation of birdsong},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{TweetyNet}},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.28.272088v1},
	doi = {10.1101/2020.08.28.272088},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Songbirds provide an excellent model system for understanding sensorimotor learning. Many analyses of learning require annotating song, but songbirds produce more songs than can be annotated by hand. Existing methods for automating annotation are challenged by variable song, like that of Bengalese finches. For particularly complex song like that of canaries, no methods exist, limiting the questions researchers can investigate. We developed an artificial neural network, TweetyNet, that automates annotation. First we benchmark the network on open datasets of Bengalese finch song, showing that TweetyNet achieves significantly lower error than a similar method, using less training data, and maintains low error across multiple days of song. We then show TweetyNet performs similarly on canary song. This accuracy allowed fully-automated analyses of datasets an order of magnitude larger than previous studies, improved the precision of statistical models of syntax, and revealed novel details of syntax in a new canary strain. Hence TweetyNet enables automated annotation and analysis of Bengalese finch and canary song that was formerly manual.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-03-30},
	journal = {bioRxiv},
	author = {Cohen, Yarden and Nicholson, David and Gardner, Timothy J.},
	month = aug,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.08.28.272088},
	file = {Full Text PDF:/home/nathan/Zotero/storage/P9P7I8R6/Cohen et al. - 2020 - TweetyNet A neural network that enables high-thro.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/JH3R5G7L/2020.08.28.272088v1.html:text/html},
}

@article{kaewtip_robust_2016,
	title = {A robust automatic birdsong phrase classification: {A} template-based approach},
	volume = {140},
	issn = {0001-4966},
	shorttitle = {A robust automatic birdsong phrase classification},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.4966592},
	doi = {10.1121/1.4966592},
	number = {5},
	urldate = {2021-03-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kaewtip, Kantapon and Alwan, Abeer and O'Reilly, Colm and Taylor, Charles E.},
	month = nov,
	year = {2016},
	note = {Publisher: Acoustical Society of America},
	pages = {3691--3701},
	file = {Snapshot:/home/nathan/Zotero/storage/6XF3FZWD/1.html:text/html},
}

@article{tan_dynamic_2015,
	title = {Dynamic time warping and sparse representation classification for birdsong phrase classification using limited training data},
	volume = {137},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.4906168},
	doi = {10.1121/1.4906168},
	number = {3},
	urldate = {2021-03-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Tan, Lee N. and Alwan, Abeer and Kossan, George and Cody, Martin L. and Taylor, Charles E.},
	month = mar,
	year = {2015},
	note = {Publisher: Acoustical Society of America},
	pages = {1069--1080},
	file = {Snapshot:/home/nathan/Zotero/storage/XT6N9GN4/1.html:text/html},
}

@article{chen_semi-automatic_2006,
	title = {Semi-automatic classification of bird vocalizations using spectral peak tracks},
	volume = {120},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.2345831},
	doi = {10.1121/1.2345831},
	number = {5},
	urldate = {2021-03-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Chen, Zhixin and Maher, Robert C.},
	month = oct,
	year = {2006},
	note = {Publisher: Acoustical Society of America},
	pages = {2974--2984},
}

@inproceedings{chu_noise_2011,
	title = {Noise robust bird song detection using syllable pattern-based hidden {Markov} models},
	doi = {10.1109/ICASSP.2011.5946411},
	abstract = {In this paper, temporal, spectral, and structural characteristics of Robin songs and syllables are studied. Syllables in Robin songs are clustered by comparing a distance measure defined as the average of aligned LPC-based frame level differences. The syllable patterns inferred from the clustering results are used for improving the acoustic modelling of a hidden Markov model (HMM)-based Robin song detector. Experiments conducted on a noisy Rocky Mountain Biological Laboratory Robin (RMBL-Robin) song corpus with more than 75 minutes of recordings show that the syllable pattern-based detector has a higher hit rate while maintaining a lower false alarm rate, compared to the detector with a general model trained from all the syllables.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chu, W. and Blumstein, D. T.},
	month = may,
	year = {2011},
	note = {ISSN: 2379-190X},
	keywords = {Training, Acoustics, Biological system modeling, Birds, Databases, Hidden Markov models, Speech},
	pages = {345--348},
	file = {IEEE Xplore Abstract Record:/home/nathan/Zotero/storage/3EIQ98UV/5946411.html:text/html;Version soumise:/home/nathan/Zotero/storage/UBQ5X3J3/Chu et Blumstein - 2011 - Noise robust bird song detection using syllable pa.pdf:application/pdf},
}

@article{markowitz_long-range_2013,
	title = {Long-range {Order} in {Canary} {Song}},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003052},
	doi = {10.1371/journal.pcbi.1003052},
	abstract = {Bird songs range in form from the simple notes of a Chipping Sparrow to the rich performance of the nightingale. Non-adjacent correlations can be found in the syntax of some birdsongs, indicating that the choice of what to sing next is determined not only by the current syllable, but also by previous syllables sung. Here we examine the song of the domesticated canary, a complex singer whose song consists of syllables, grouped into phrases that are arranged in flexible sequences. Phrases are defined by a fundamental time-scale that is independent of the underlying syllable duration. We show that the ordering of phrases is governed by long-range rules: the choice of what phrase to sing next in a given context depends on the history of the song, and for some syllables, highly specific rules produce correlations in song over timescales of up to ten seconds. The neural basis of these long-range correlations may provide insight into how complex behaviors are assembled from more elementary, stereotyped modules.},
	language = {en},
	number = {5},
	urldate = {2021-03-31},
	journal = {PLOS Computational Biology},
	author = {Markowitz, Jeffrey E. and Ivie, Elizabeth and Kligler, Laura and Gardner, Timothy J.},
	month = may,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Acoustics, Birds, Animal behavior, Bird song, Canaries, Entropy, Syllables, Syntax},
	pages = {e1003052},
	file = {Full Text PDF:/home/nathan/Zotero/storage/U4FL4KCM/Markowitz et al. - 2013 - Long-range Order in Canary Song.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/BEY6E58R/article.html:text/html},
}

@article{pearre_fast_2017,
	title = {A fast and accurate zebra finch syllable detector},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181992},
	doi = {10.1371/journal.pone.0181992},
	abstract = {The song of the adult male zebra finch is strikingly stereotyped. Efforts to understand motor output, pattern generation, and learning have taken advantage of this consistency by investigating the bird’s ability to modify specific parts of song under external cues, and by examining timing relationships between neural activity and vocal output. Such experiments require that precise moments during song be identified in real time as the bird sings. Various syllable-detection methods exist, but many require special hardware, software, and know-how, and details on their implementation and performance are scarce. We present an accurate, versatile, and fast syllable detector that can control hardware at precisely timed moments during zebra finch song. Many moments during song can be isolated and detected with false negative and false positive rates well under 1\% and 0.005\% respectively. The detector can run on a stock Mac Mini with triggering delay of less than a millisecond and a jitter of σ ≈ 2 milliseconds.},
	language = {en},
	number = {7},
	urldate = {2021-03-31},
	journal = {PLOS ONE},
	author = {Pearre, Ben and Perkins, L. Nathan and Markowitz, Jeffrey E. and Gardner, Timothy J.},
	month = jul,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Neural networks, Birds, Bird song, Syllables, Computer hardware, Computer software, Jitter, Zebra finch},
	pages = {e0181992},
	file = {Full Text PDF:/home/nathan/Zotero/storage/WEDFV4D4/Pearre et al. - 2017 - A fast and accurate zebra finch syllable detector.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/VLQ6FPFF/article.html:text/html},
}

@article{daou_computational_2012,
	title = {A computational tool for automated large-scale analysis and measurement of bird-song syntax},
	volume = {210},
	issn = {0165-0270},
	url = {https://www.sciencedirect.com/science/article/pii/S0165027012002841},
	doi = {10.1016/j.jneumeth.2012.07.020},
	abstract = {We present computer software for automated, high throughput, quantitative syllable-level analysis of bird song syntax. The primary advantage of our tool is the ease and effectiveness it provides in quantifying syllable sequence and performing a comparison of syllable sequence from one day of singing with one or more other days of singing. The software utilizes the output of the Feature Batch module in Sound Analysis Pro (Tchernichovski et al., 2000) that can be used to measure the temporal and spectral features of each syllable produced during a day of singing. We use these measurements to identify individual syllables based on their temporal and spectral properties and then identify transition probabilities among syllables to determine changes in syntax. This quantifies the ordering of syllables in songs and the frequency with which subsequences appear. Moreover, the software computes the linearity, consistency, and stereotypy scores for every bout presented as well as descriptive statistics for each of these measures for each day of singing. We also report statistical measures that the software utilizes (the Kullback–Leibler distance and the sequence entropy) to quantify the degree of dissimilarity between sequences of syllable transitions. Our tool is useful for comparing the syntactic structure of songs produced by a bird prior to and after a manipulation such as ablation of part of the vocal motor pathway or infusion of pharmacological agents, or for assessing the degree of individual variation in syntactic structure across populations of birds.},
	language = {en},
	number = {2},
	urldate = {2021-03-31},
	journal = {Journal of Neuroscience Methods},
	author = {Daou, Arij and Johnson, Frank and Wu, Wei and Bertram, Richard},
	month = sep,
	year = {2012},
	keywords = {Computer software, Behavioral analysis, Birdsong, Sequential behavior},
	pages = {147--160},
	file = {ScienceDirect Full Text PDF:/home/nathan/Zotero/storage/P9SUB3AW/Daou et al. - 2012 - A computational tool for automated large-scale ana.pdf:application/pdf;ScienceDirect Snapshot:/home/nathan/Zotero/storage/XQGJN7YT/S0165027012002841.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2021-03-31},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Full Text PDF:/home/nathan/Zotero/storage/UMVV8VED/Hochreiter et Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf},
}

@article{koumura_automatic_2016,
	title = {Automatic {Recognition} of {Element} {Classes} and {Boundaries} in the {Birdsong} with {Variable} {Sequences}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0159188},
	doi = {10.1371/journal.pone.0159188},
	abstract = {Researches on sequential vocalization often require analysis of vocalizations in long continuous sounds. In such studies as developmental ones or studies across generations in which days or months of vocalizations must be analyzed, methods for automatic recognition would be strongly desired. Although methods for automatic speech recognition for application purposes have been intensively studied, blindly applying them for biological purposes may not be an optimal solution. This is because, unlike human speech recognition, analysis of sequential vocalizations often requires accurate extraction of timing information. In the present study we propose automated systems suitable for recognizing birdsong, one of the most intensively investigated sequential vocalizations, focusing on the three properties of the birdsong. First, a song is a sequence of vocal elements, called notes, which can be grouped into categories. Second, temporal structure of birdsong is precisely controlled, meaning that temporal information is important in song analysis. Finally, notes are produced according to certain probabilistic rules, which may facilitate the accurate song recognition. We divided the procedure of song recognition into three sub-steps: local classification, boundary detection, and global sequencing, each of which corresponds to each of the three properties of birdsong. We compared the performances of several different ways to arrange these three steps. As results, we demonstrated a hybrid model of a deep convolutional neural network and a hidden Markov model was effective. We propose suitable arrangements of methods according to whether accurate boundary detection is needed. Also we designed the new measure to jointly evaluate the accuracy of note classification and boundary detection. Our methods should be applicable, with small modification and tuning, to the songs in other species that hold the three properties of the sequential vocalization.},
	language = {en},
	number = {7},
	urldate = {2021-03-31},
	journal = {PLOS ONE},
	author = {Koumura, Takuya and Okanoya, Kazuo},
	month = jul,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Neural networks, Birds, Hidden Markov models, Speech, Bird song, Syntax, Markov models, Vocalization},
	pages = {e0159188},
	file = {Full Text PDF:/home/nathan/Zotero/storage/I3LY8WVS/Koumura et Okanoya - 2016 - Automatic Recognition of Element Classes and Bound.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/TAA9HF62/article.html:text/html},
}

@article{cazala_neuronal_2019,
	title = {Neuronal {Encoding} in a {High}-{Level} {Auditory} {Area}: {From} {Sequential} {Order} of {Elements} to {Grammatical} {Structure}},
	volume = {39},
	copyright = {Copyright © 2019 the authors},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Neuronal {Encoding} in a {High}-{Level} {Auditory} {Area}},
	url = {https://www.jneurosci.org/content/39/31/6150},
	doi = {10.1523/JNEUROSCI.2767-18.2019},
	abstract = {Sensitivity to the sequential structure of communication sounds is fundamental not only for language comprehension in humans but also for song recognition in songbirds. By quantifying single-unit responses, we first assessed whether the sequential order of song elements, called syllables, in conspecific songs is encoded in a secondary auditory cortex-like region of the zebra finch brain. Based on a habituation/dishabituation paradigm, we show that, after multiple repetitions of the same conspecific song, rearranging syllable order reinstated strong responses. A large proportion of neurons showed sensitivity to song context in which syllables occurred providing support for the nonlinear processing of syllable sequences. Sensitivity to the temporal order of items within a sequence should enable learning its underlying structure, an ability considered a core mechanism of the human language faculty. We show that repetitions of songs that were ordered according to a specific grammatical structure (i.e., ABAB or AABB structures; A and B denoting song syllables) led to different responses in both anesthetized and awake birds. Once responses were decreased due to song repetitions, the transition from one structure to the other could affect the firing rates and/or the spike patterns. Our results suggest that detection was based on local differences rather than encoding of the global song structure as a whole. Our study demonstrates that a high-level auditory region provides neuronal mechanisms to help discriminate stimuli that differ in their sequential structure.
SIGNIFICANCE STATEMENT Sequence processing has been proposed as a potential precursor of language syntax. As a sequencing operation, the encoding of the temporal order of items within a sequence may help in recognition of relationships between adjacent items and in learning the underlying structure. Taking advantage of the stimulus-specific adaptation phenomenon observed in a high-level auditory region of the zebra finch brain, we addressed this question at the neuronal level. Reordering elements within conspecific songs reinstated robust responses. Neurons also detected changes in the structure of artificial songs, and this detection depended on local transitions between adjacent or nonadjacent syllables. These findings establish the songbird as a model system for deciphering the mechanisms underlying sequence processing at the single-cell level.},
	language = {en},
	number = {31},
	urldate = {2021-03-31},
	journal = {Journal of Neuroscience},
	author = {Cazala, Aurore and Giret, Nicolas and Edeline, Jean-Marc and Negro, Catherine Del},
	month = jul,
	year = {2019},
	pmid = {31147525},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {artificial grammar, auditory perception, multielectrode, sequence, songbirds},
	pages = {6150--6161},
	file = {Full Text PDF:/home/nathan/Zotero/storage/FV9ILS26/Cazala et al. - 2019 - Neuronal Encoding in a High-Level Auditory Area F.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/469EICNG/6150.html:text/html},
}

@article{cohen_hidden_2020,
	title = {Hidden neural states underlie canary song syntax},
	volume = {582},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2397-3},
	doi = {10.1038/s41586-020-2397-3},
	abstract = {Coordinated skills such as speech or dance involve sequences of actions that follow syntactic rules in which transitions between elements depend on the identities and order of past actions. Canary songs consist of repeated syllables called phrases, and the ordering of these phrases follows long-range rules1 in which the choice of what to sing depends on the song structure many seconds prior. The neural substrates that support these long-range correlations are unknown. Here, using miniature head-mounted microscopes and cell-type-specific genetic tools, we observed neural activity in the premotor nucleus HVC2–4 as canaries explored various phrase sequences in their repertoire. We identified neurons that encode past transitions, extending over four phrases and spanning up to four seconds and forty syllables. These neurons preferentially encode past actions rather than future actions, can reflect more than one song history, and are active mostly during the rare phrases that involve history-dependent transitions in song. These findings demonstrate that the dynamics of HVC include ‘hidden states’ that are not reflected in ongoing behaviour but rather carry information about prior actions. These states provide a possible substrate for the control of syntax transitions governed by long-range rules.},
	language = {en},
	number = {7813},
	urldate = {2021-03-31},
	journal = {Nature},
	author = {Cohen, Yarden and Shen, Jun and Semu, Dawit and Leman, Daniel P. and Liberti, William A. and Perkins, L. Nathan and Liberti, Derek C. and Kotton, Darrell N. and Gardner, Timothy J.},
	month = jun,
	year = {2020},
	note = {Number: 7813
Publisher: Nature Publishing Group},
	pages = {539--544},
	file = {Snapshot:/home/nathan/Zotero/storage/MULMUKDW/s41586-020-2397-3.html:text/html;Version acceptée:/home/nathan/Zotero/storage/5NYTYCS2/Cohen et al. - 2020 - Hidden neural states underlie canary song syntax.pdf:application/pdf},
}

@article{sainburg_latent_2020,
	title = {Latent space visualization, characterization, and generation of diverse vocal communication signals},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/870311v2},
	doi = {10.1101/870311},
	abstract = {{\textless}h3{\textgreater}ABSTRACT{\textless}/h3{\textgreater} {\textless}p{\textgreater}Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species’ vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present here a set of computational methods that center around projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from data. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates, enabling high-powered comparative analyses of unbiased acoustic features in the communicative repertoires across species. Latent projections uncover complex features of data in visually intuitive and quantifiable ways. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication. Finally, we show how systematic sampling from latent representational spaces of vocalizations enables comprehensive investigations of perceptual and neural representations of complex and ecologically relevant acoustic feature spaces.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-03-31},
	journal = {bioRxiv},
	author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
	month = jan,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {870311},
	file = {Full Text PDF:/home/nathan/Zotero/storage/GN38JGWH/Sainburg et al. - 2020 - Latent space visualization, characterization, and .pdf:application/pdf},
}

@article{leitner_syllable_2004,
	title = {Syllable repertoire and the size of the song control system in captive canaries ({Serinus} canaria)},
	volume = {60},
	issn = {1097-4695},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/neu.10331},
	doi = {https://doi.org/10.1002/neu.10331},
	abstract = {In songbirds, there is considerable interest in relationships between song structure and the size of the song control system in the forebrain. In male canaries, earlier studies have reported that repertoire size increased with age, and positive correlations were obtained between repertoire size and the volume of song control nuclei such as high vocal center (HVC). Here we investigate whether age has an effect upon both the song structure and the morphology of two song control nuclei [HVC and robustus archistriatalis (RA)] that are important in song production. We recorded songs from an aviary population of 1- and 2-year-old male domesticated canaries. We found that repertoire size, number of sexually attractive (sexy) syllables, and size of song nuclei did not differ between 1- and 2-year-old males. Neither did we find significant correlations between syllable repertoire size and the size of the song control nuclei. However, HVC size was positively correlated with the proportion of sexy syllables in the repertoires of 2-year-old males. Some older males may enhance vocal performance by modifying the control of syllables rather than by increasing repertoire size or neural space. © 2004 Wiley Periodicals, Inc. J Neurobiol 60: 21–27, 2004},
	language = {en},
	number = {1},
	urldate = {2021-04-07},
	journal = {Journal of Neurobiology},
	author = {Leitner, Stefan and Catchpole, Clive K.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/neu.10331},
	keywords = {age, canary, HVC, sexy syllables, song},
	pages = {21--27},
	file = {Full Text PDF:/home/nathan/Zotero/storage/5N967CCF/Leitner et Catchpole - 2004 - Syllable repertoire and the size of the song contr.pdf:application/pdf;Snapshot:/home/nathan/Zotero/storage/MU2B2ELF/neu.html:text/html},
}

@article{anderson_templatebased_1996,
	title = {Template‐based automatic recognition of birdsong syllables from continuous recordings},
	volume = {100},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.415968},
	doi = {10.1121/1.415968},
	number = {2},
	urldate = {2021-04-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Anderson, Sven E. and Dave, Amish S. and Margoliash, Daniel},
	month = aug,
	year = {1996},
	note = {Publisher: Acoustical Society of America},
	pages = {1209--1219},
	file = {Snapshot:/home/nathan/Zotero/storage/QV5TFSYJ/1.html:text/html},
}

@article{gardner_freedom_2005,
	title = {Freedom and {Rules}: {The} {Acquisition} and {Reprogramming} of a {Bird}'s {Learned} {Song}},
	shorttitle = {Freedom and {Rules}},
	url = {/paper/Freedom-and-Rules%3A-The-Acquisition-and-of-a-Bird%27s-Gardner-Naef/d1499a8b11bda92b2b0e19103cc39292dbc05ad0},
	abstract = {Canary song is hierarchically structured: Short stereotyped syllables are repeated to form phrases, which in turn are arranged to form songs. This structure occurs even in the songs of young isolates, which suggests that innate rules govern canary song development. However, juveniles that had never heard normal song imitated abnormal synthetic songs with great accuracy, even when the tutor songs lacked phrasing. As the birds matured, imitated songs were reprogrammed to form typical canary phrasing. Thus, imitation and innate song constraints are separate processes that can be segregated in time: freedom in youth, rules in adulthood.},
	language = {en},
	urldate = {2021-04-07},
	journal = {undefined},
	author = {Gardner, T. and Naef, F. and Nottebohm, F.},
	year = {2005},
	file = {Snapshot:/home/nathan/Zotero/storage/SBLXLS69/d1499a8b11bda92b2b0e19103cc39292dbc05ad0.html:text/html;Gardner2005science.pdf:/home/nathan/Zotero/storage/892A5A37/Gardner2005science.pdf:application/pdf},
}

@article{waser_song_1977,
	title = {Song learning in canaries},
	volume = {91},
	issn = {0021-9940(Print)},
	doi = {10.1037/h0077299},
	abstract = {Studied vocal learning in roller canaries (an inbred strain of Belgian "Wasserschlager") by comparing song structure in young males and in other birds to which they were exposed. When reared with fathers to sexual maturity, sons learned extensively from their songs, sharing from 76\% to 91\% of song syllables with them. Randomly chosen individuals shared from 12\% to 21\% of syllable types. Extensive learning was also demonstrated in young males exposed to adult male song through a loudspeaker, without social contact, from weaning to maturity. The level of syllable sharing with the model, ranging from 58\% to 82\%, is suggestively but not significantly lower than that achieved with social contact. Sibling males also engage in song learning from one another. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Journal of Comparative and Physiological Psychology},
	author = {Waser, Mary S. and Marler, Peter},
	year = {1977},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Animal Social Behavior, Animal Vocalizations, Canaries, Male Animals},
	pages = {1--7},
	file = {Snapshot:/home/nathan/Zotero/storage/HPWG4A9D/1977-22803-001.html:text/html},
}
